\documentclass[muchmore,11pt]{article}%
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{named}
\usepackage{flexcite}
\usepackage{comment}
\usepackage{litprog}%
\setcounter{MaxMatrixCols}{30}
%TCIDATA{OutputFilter=latex2.dll}
%TCIDATA{Version=5.50.0.2960}
%TCIDATA{CSTFile=Literate Programming Article.cst}
%TCIDATA{Created=Friday, December 17, 2004 15:26:57}
%TCIDATA{LastRevised=Thursday, August 11, 2016 17:29:37}
%TCIDATA{<META NAME="GraphicsSave" CONTENT="32">}
%TCIDATA{<META NAME="SaveForMode" CONTENT="1">}
%TCIDATA{BibliographyScheme=BibTeX}
%TCIDATA{<META NAME="DocumentShell" CONTENT="Articles\Literate Programming Article">}
%TCIDATA{Language=American English}
%BeginMSIPreambleData
\providecommand{\U}[1]{\protect\rule{.1in}{.1in}}
%EndMSIPreambleData
\newtheorem{theorem}{Theorem}
\newtheorem{acknowledgement}[theorem]{Acknowledgement}
\newtheorem{algorithm}[theorem]{Algorithm}
\newtheorem{axiom}[theorem]{Axiom}
\newtheorem{case}[theorem]{Case}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{conclusion}[theorem]{Conclusion}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{criterion}[theorem]{Criterion}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{solution}[theorem]{Solution}
\newtheorem{summary}[theorem]{Summary}
\newenvironment{proof}[1][Proof]{\textbf{#1.} }{\ \rule{0.5em}{0.5em}}
\begin{document}

\title{Proof-Outline Logic (Concurrent Programming Edition)}
\author{Theodore S\ Norvell\\Electrical and Computer Engineering\\Memorial University}
\date{Draft typeset
%TCIMACRO{\TeXButton{Today}{\today}}%
%BeginExpansion
\today
%EndExpansion
}
\maketitle

\begin{abstract}
An introduction to proof outlines, compiled as background reading for Engi
7894 Concurrent Programming and Engi 9869 Advanced Concurrent Programming.

\end{abstract}

\textbf{Note on editions:}\ This is the Concurrent Programming edition,
designed to support MUN courses Engi-7894 and Engi-9869.\ The first part of
this note ---the part up to the section entitled Concurrent Programming---
also appears in an edition for MUN\ course Engi-6892 Algorithms:\ Correctness
and Complexity. The content is the same, but the notation is a bit different.
The notation used in this edition better matches the notation used in Andrews'
text book \cite{Andrews2000}.

\section{Preface}

This note provides background on assertions and the use of assertions in
designing correct programs.

The ideas presented are mainly due, for the sequential programming part, to
Floyd \cite{Floyd670} and Hoare \cite{Hoare690}, and, for the concurrent
programming part, to Lamport \cite{Lamport-1977} and Gries and Owicki
\cite{OwickiGries-1976-aptpp,OwickiGries-1976-vpppaa}. Blikle
\cite{Blikle1979} presented an early version of proof-outline logic for
sequential programs. An excellent and detailed study of Owicki/Gries theory is
to be found in \cite{FeijenVanGasteren990}, where the theory is expanded from
a set of rules for checking proofs of parallel programs to a method for
developing parallel programs and their proofs.

Sections \ref{assertions} and \ref{seq} deal with sequential programming. As
such they are an elaboration of Hoare's excellent `Axiomatic basis' paper
\cite{Hoare690}. I\ suggest reading Hoare's paper first. For sequential
programs, proof-outline logic is just like Hoare logic except that commands
contain internal assertions. Section \ref{conc} then extends the rules to
cover concurrent programs.

Section \ref{formalization} describes the syntax and semantics a bit more
formally than the preceding sections. It is entirely optional reading.

Section \ref{simple} gives some example and shows some handy tricks for
showing noninterference. Section \ref{global} discusses global invariants,
which are assertions that are true throughout the execution of a concurrent
program. It gives a handy abbreviation that saves you from having to write
global invariants over and over, and, more importantly, concludes with an
example of proving a communication protocol. The final two sections show
applications of global invariants. Section \ref{ghost} shows how introducing
extra variables can simplify proofs and Section \ref{trans} shows how to use a
sort of coordinate transformation to change the set of variables used in a program.

The programming notation that I\ use is based on that in Gregory Andrews' text
\cite{Andrews2000}, which is based on C. I'll make a few \textquotedblleft
improvements\textquotedblright\ to Andrews' notation; I will mention them as I
go along; I summarize them here:

\begin{itemize}
\item Andrews uses the Fortran/C/Java notation for assignment: $v=E;$. I use
the Algol/Pascal/Ada notation: $v:=E;$.

\item Andrews uses the C/Java notation for equality: $E==F$. I use the
mathematical notation: $E=F$.

\item Andrews brackets assertions with \textquotedblleft\#\#\textquotedblright%
\ and the end of the line\ (eol). I do this too, but as an alternative, I'll
sometimes bracket them with \textquotedblleft\{\textquotedblright\ and
\textquotedblleft\}\textquotedblright, which is the tradition in Hoare logic.
The latter notation is particularly useful when you don't want to be forced to
put line breaks in the middle of a formula.

\item Andrews groups commands with \textquotedblleft\{\textquotedblright\ and
\textquotedblleft\}\textquotedblright, as in C and Java. I\ do that too, but
to avoid confusion with assertions, I'll sometimes use \textquotedblleft%
(\textquotedblright\ and \textquotedblleft)\textquotedblright. For larger
examples I'll use \#\#/eol for assertions with \{/\} for grouping commands;
for smaller examples and in the theory, I'll use \{/\} for assertions with (/)
for grouping commands.
\end{itemize}

\section{Conditions and assertions\label{assertions}}

\subsection{Assertions}

A \textbf{condition} is a boolean expression with free variables chosen from
the state variables of a program. For example if we have variables

\begin{code}
int x ;

int y ;
\end{code}

Then the following are all examples of conditions%
\begin{align*}
x  &  <y\\
x  &  =y\\
x+y  &  =0\\
x  &  \geq0\\
x  &  \geq0\wedge x+y=0\qquad\text{.}%
\end{align*}


A condition that is expected to be true every time execution passes a
particular point in a program is called an \textbf{assertion}. In this course,
assertions are preceded by \#\# and are followed by an end-of-line like
this:\footnote{This fragment uses both the symbol $:=$ and $=$.This is one of
those \textquotedblleft notational improvements\textquotedblright%
\ I\ mentioned. I will use $:=$ for assignment and either $=$ or $==$ for
equality when writing pseudo-code. In C, C++, and Java, $=$ is used for
assignment and $==$ for equality. Andrews follows the C/C++/Java convention.
If you want to be on the safe side of any possible misunderstanding, you can
use $:=$ for assignment and $==$ for equality.}

\begin{code}
int x ;

int y ;

x := 5 ;

y := -5 ;

\#\#\ $x\geq0\wedge x+y=0$

z := x+z ;
\end{code}

Sometimes I'll write assertions inside curly brackets like this:

\begin{code}
int x ; int y ; x := 5 ; y := -5 ; $\left\{  x\geq0\wedge x+y=0\right\}  $ z
:= x+y ;
\end{code}

\subsection{Assertions in C, C++, and Java}

If one is programming in C or C++, then assertions may be written either as
comments or using the assert macro from the standard C\ library. E.g.

\begin{code}
\#include
%TCIMACRO{\TEXTsymbol{<}}%
%BeginExpansion
$<$%
%EndExpansion
assert.h%
%TCIMACRO{\TEXTsymbol{>}}%
%BeginExpansion
$>$%
%EndExpansion


...

\textbf{int} x ;

\textbf{int} y ;

x = 5 ;

y = -5 ;

assert(\ x
%TCIMACRO{\TEXTsymbol{>}}%
%BeginExpansion
$>$%
%EndExpansion
=0 \&\&\ x+y == 0 )\ ;
\end{code}

\noindent Assertions written using the \textsf{assert} macro will be evaluated
at run time and the program will come to a grinding halt, should the assertion
ever evaluate to false.\footnote{By using a different include file, one can,
of course, make the action followed on a false assertion be whatever you like.
For example, in a desktop application, one might cause all files to be saved
and an error report to be assembled and e-mailed back to the developers; in an
embedded system, one might cause the system to go into safe mode. If you
program in C or C++, I\ strongly suggest redefining the assert macro in a way
that suits your application. And use it!}

In Java one easily create one's own \textsf{Assert} class with a
\textsf{check} method in it.\footnote{As of Java 1.4, there is actually an
\textsf{assert} keyword in Java. However I don't recommend its use. Assertion
checking is turned off by default in most (if not all) JVMs. This can be
compared to removing the seat belts from a car's design once it goes into
production. In my own work I use my own assertion checking class. I\ recommend
you do the same.}

\begin{code}
\textbf{public} \textbf{class} Assert \{

\begin{indent}
\item \textbf{public static void} check( \textbf{boolean} b ) \{

\begin{indent}
\item if( !b ) \{ \textbf{throw} \textbf{new} java.lang.AssertionError() ; \}
\end{indent}

\item \}
\end{indent}

\}
\end{code}

\noindent This can be used in your code as follows:

\begin{code}
\textbf{int} x ;

\textbf{int} y ;

x = 5 ;

y = -5 ;

Assert.check(\ x
%TCIMACRO{\TEXTsymbol{>}}%
%BeginExpansion
$>$%
%EndExpansion
=0 \&\&\ x+y == 0 )\ ;
\end{code}

\subsubsection{Be an assertive programmer}

Using assertions has several benefits.

\begin{itemize}
\item In the design process, they help you articulate what conditions you
expect to be true at various points in program execution.

\item In testing, executable assertions can help you identify errors in your
code or in your design.

\item In execution, executable assertions ---combined with a recovery
mechanism--- can help make your program more fault tolerant.

\item Assertions provide valuable documentation. Executable assertions are
more valuable than comments, as they are more likely to be accurate.
\end{itemize}

Whether to code assertions as comments or as executable checks is a question
that depends on the local conditions of the project you are working on.
Sometimes it has to be answered on a case-by-case basis. In this course we
will concentrate on the use of assertions in the design process, rather than
on their (nevertheless important)\ uses in testing, documentation, and in
making systems fault-tolerant. My general advice is to make assertions
executable as much as is practical.\footnote{In concurrent programming there
is an additional complication in making assertions executable, namely that
they should be evaluated atomically. Consider the assertion%
\[
x=0\vee y=0
\]
if we evaluate this in parallel with the following sequence of assignments%
\[
x:=0;y:=1;
\]
it is possible that the assertion will evaluate to false even though there is
no time at which it is in fact false.
\par
This problem can be solved by evaluating assertions only when the thread has
exclusive access to the data they refer to.}

\subsection{Substitutions}

Sometimes it is useful to create a new condition by replacing all free
occurrences of a variable $x$ in a condition $P$ by an expression $(E)$. We
write $P_{x\leftarrow E}$ for the new condition.\footnote{A variety different
notations are used by authors for substitution. In other courses, I usually
use $P[x:E]$. Here I am following Andrews's book.} For example%
\[%
\begin{tabular}
[c]{lll}%
$\left(  x\geq0\wedge x+y=0\right)  _{x\leftarrow z}$ & is & $\left(
z\right)  \geq0\wedge\left(  z\right)  +y=0$\\
$\left(  x\geq0\wedge x+y=0\right)  _{x\leftarrow x+y}$ & is & $\left(
x+y\right)  \geq0\wedge\left(  x+y\right)  +y=0$\\
$\left(  2y=5\right)  _{y\leftarrow y+z}$ & is & $2(y+z)=5\qquad\text{.}$%
\end{tabular}
\ \ \ \ \ \
\]
It is useful to extend this notation to allow the simultaneous substitution
for more than one variable. For example%
\[
\left(  x\geq0\wedge x+y=0\right)  _{x,y\leftarrow z,x}\text{ is }\left(
z\right)  \geq0\wedge\left(  z\right)  +\left(  x\right)  =0
\]
Usually we omit the parentheses in contexts where they are not required.

\subsection{Propositional and predicate logic}

In this section, I will review a little bit of propositional and predicate
logic. We use notations $\lnot$ (not), $=$ (equality), $\wedge$ (and), $\vee$,
(or), and $\Rightarrow$ (implication). Precedence between the operators is in
the same order. Some of the laws of propositional logic that will be useful in
this course are%
\[%
\begin{tabular}
[c]{ll}%
$\left(  P\Rightarrow Q\right)  =\left(  \lnot P\vee Q\right)  $ & Material
implication\\
$\left(  \mathit{true}\Rightarrow P\right)  =P$ & Identity\\
$\mathit{false}\Rightarrow P$ & Antidomination\\
$P\Rightarrow P$ & Reflexivity\\
$P\Rightarrow\mathit{true}$ & Domination\\
$\left(  P\Rightarrow(Q\Rightarrow R)\right)  =\left(  P\wedge Q\Rightarrow
R\right)  $ & Shunting\\
$\left(  P0\Rightarrow Q\right)  \Rightarrow\left(  P0\wedge P1\Rightarrow
Q\right)  $ & Subsetting the antecedent\\
$\left(  P\Rightarrow Q\wedge R\right)  =\left(  P\Rightarrow Q\right)
\wedge\left(  P\Rightarrow R\right)  $ & Distributivity
\end{tabular}
\ \ \
\]


Also frequently useful are the \textbf{one-point laws}, which let you make use
information from equalities. $E$ and $F$ range over expressions of any type,
$v$ is a variable.
\begin{align*}
\left(  E=F\Rightarrow P_{v\leftarrow E}\right)   &  =\left(  E=F\Rightarrow
P_{v\leftarrow F}\right) \\
\left(  E=F\wedge P_{v\leftarrow E}\right)   &  =\left(  E=F\wedge
P_{v\leftarrow F}\right)
\end{align*}
To see that these are true, consider the case where $E=F$ and then the case
where $E\neq F$.

For example%
\[
x=X\wedge y=Y\Rightarrow x^{y}=X^{Y}%
\]
simplifies, using one-point (and shunting), to%
\[
x=X\wedge y=Y\Rightarrow X^{Y}=X^{Y}%
\]
which then simplifies to%
\[
x=X\wedge y=Y\Rightarrow\mathit{true}%
\]
which is then true.

Any condition that is true for all assignments of values to its free
variables, is called a \textbf{universally true} formula. For example,
(assuming $x$ and $y$ and $z$ are integer variables) the following are all
universally true%
\begin{align*}
&  \left.  2+2=4\right. \\
&  \left.  x<y\wedge y<z\Rightarrow x<z\right. \\
&  \left.  x+1>x\right.
\end{align*}
However, $x^{2}+y^{2}=z^{2}$ is not universally true, because there is an
assignment for which it is not true; for example $3^{2}+4^{2}=6^{2}$ is not true.

Whether a condition is universally true may depend on the types ascribed to
its variables. For example, if we are using Java and $x$ has type
\textbf{int}, then, by the rules of the Java language, the value of $x+1,$
when $x$ is $2^{31}-1,$ is $-2^{31}$; so $x+1>x$ is not universally true when
$x$ represents a Java \textbf{int} and $+$ is interpreted as Java \textbf{int} addition.

\begin{comment}
    Note that Dafny uses short circuiting three valued logic, so it might be a good idea to change POL to
    do the same.
\end{comment}

Sometimes expressions are undefined, for example, supposing $x$ and $y$ are
rational variables, $x/y$ is undefined when $y$ is $0$, so this raises the
question of whether $1=x/x$ is universally true. We'll say that such an
expression is not universally true. However, the expression $x\neq
0\Rightarrow1=x/x$ is universally true; if we consider the case of $x=0$ , we
have $\mathrm{false}\Rightarrow?$, and applying the principle of
antidomination, this is $\mathrm{true}$. Using $?$ to represent an unknown or
undefined truth value, we can fill in truth tables for the propositional logic
as follows%
\begin{align*}
&
\begin{tabular}
[c]{|l||l|}\hline
$\lnot$ & \\\hline\hline
false & true\\\hline
? & ?\\\hline
true & false\\\hline
\end{tabular}
\qquad%
\begin{tabular}
[c]{|l||l|l|l|}\hline
$\wedge$ & false & ? & true\\\hline\hline
false & false & false & false\\\hline
? & false & ? & ?\\\hline
true & false & ? & true\\\hline
\end{tabular}
\\
&
\begin{tabular}
[c]{|l||l|l|l|}\hline
$\vee$ & false & ? & true\\\hline\hline
false & false & ? & true\\\hline
? & ? & ? & true\\\hline
true & true & true & true\\\hline
\end{tabular}
\qquad%
\begin{tabular}
[c]{|l||l|l|l|}\hline
$\Rightarrow$ & false & ? & true\\\hline\hline
false & true & true & true\\\hline
? & ? & ? & true\\\hline
true & false & ? & true\\\hline
\end{tabular}
\end{align*}


\section{Sequential programming\label{seq}}

\subsection{Contracts}

A specification for a component indicates the operating conditions (i.e. the
conditions under which the component is expected to operate) and the function
of the component (i.e. the relationship between the component's inputs and
outputs). For example we might specify a resistor by saying that the
relationship between the voltage and current across the resistor is given by%
\[
953I\leq V\leq1050I\qquad\text{,}%
\]
provided%
\[
0\leq V\leq+10\qquad\text{.}%
\]
The latter formula gives the operating conditions, the former the relation
between inputs and outputs.

In programming, we can use a pair of assertions as a specification or
\textquotedblleft contract\textquotedblright\ for a command or subroutine. The
first assertion is the so-called precondition, it specifies the operating
conditions, that is, the state of the program when the command begins
operation. The second assertion specifies the state of the program when (and
if) the command ends operation. For example, the following pair of conditions
specifies a command that results in $x$ being assigned the value 5, provided
that $y$ is initially $4$:%
\[
\lbrack y=4,x=5]\qquad\text{,}%
\]
where $x$ and $y$ are understood to be state variables of type \textsf{int}.
One solution to this particular contract is

\begin{code}
\#\# $y=4$

$x:=y+1$ ;

\#\# $x=5\qquad$.
\end{code}

\noindent Such a triple, consisting of a precondition, a command, and a
postcondition, is called a \textbf{Hoare triple} after C.A.R. Hoare, who
introduced the idea to programming.\footnote{Traditionally Hoare triples are
written with the assertions in braces. So we would, traditionally, write
\[
\{y=4\}\quad x:=y+1\quad\{x=5\}
\]
I'm going to use braces sometimes and the \#\# convention at others.} Here is
another solution:

\begin{code}
\#\# $y=4$

$x:=5$ ;

\#\# $x=5\qquad$.
\end{code}

\noindent Here is one more:

\begin{code}
\#\# $y=4$

$y:=y+1$ ;

$x:=y$ ;

\#\# $x=5\qquad$.
\end{code}

\noindent Nothing in the contract says that $y$ must not change!

If you do want to specify that a variable does not change, then `constants'
can be used. Constants are conventionally written with capital letters. The
following contract specifies that, provided $y$ is initially less than 100,
$y$ must not change and the final value of $x$ must be larger than that of
$y$:%
\[
\lbrack y<\mathrm{100}\wedge y=Y,y=Y\wedge x>y]
\]
where it is understood that $x$ and $y$ are state variables of type
\textsf{int},\textsf{ }while $Y$ is a constant\footnote{The word `constant' is
the traditional term to use. In the mathematical sense, $Y$ is a variable. We
use the term `constant' to distinguish such mathematical variables from
\textquotedblleft program variables\textquotedblright\ which refer to
components of the program state. The point is that $Y\ $can't be changed by
the execution of the program so $Y$ represents the same value in both the
precondition and in the postcondition. Since the precondition implies $y=Y$
and the postcondition implies $y=Y$, it is clear that $y$ has the same value
in the final state as it has in the initial state.
\par
It is so common to use constants equated to the initial values of variables
that the following convention has evolved. The notation $e_{o}$ is used to
refer to the value of expression $e$ in the original state. Thus this contract
could be written as%
\[
\left[  y<100,y=y_{o}\wedge x>y\right]
\]
} of type \textsf{int}.

\subsection{Partial correctness}

Consider a Hoare triple $\{P\}\;S\;\{Q\}$ or equivalently

\begin{code}
\#\# $P$

$S$

\#\# $Q$
\end{code}

\noindent We define that the triple is \textbf{partially correct} if and only
if, for all possible values of all constants, whenever the execution of $S$ is
started in a state satisfying $P$, the execution of $S$ does not crash and can
only end in a state satisfying $Q$.

Note that the definition of partial correctness does not require that $S$
should terminate. Thus the following triple is partially correct even if we
interpret $x$ to have the type $\mathbb{Z}$, that is, to range over all
mathematical integers%
\[
\{\mathrm{true}\}\quad\mathbf{while}(x\neq0)\;x:=x-1;\quad\{x=0\}\qquad
\text{.}%
\]
If we consider initial states where $x$ is positive or zero, then eventually
the while-loop will terminate and the program will halt in a state where
$x=0$, satisfying the postcondition. If we start the while-loop in an initial
state where $x$ is negative, then the while-loop will never terminate and so
execution \textquotedblleft can only end in a state
satisfying\textquotedblright\ the postcondition by virtue of the fact it never
ends at all!

We write $\vdash\{P\}\;S\;\{Q\}$ to mean \textquotedblleft$\{P\}\;S\;\{Q\}$ is
partially correct\textquotedblright

In concurrent programming, we are often interested in processes that do not
terminate (e.g., in embedded systems) so dealing with partial correctness is
an appropriate and desirable thing to do. If termination is important, we can
deal with it as a separate concern. From here on, we won't be worried about
any other kind of correctness, so we will just say \textquotedblleft
correct\textquotedblright.

\subsection{Some examples of assignments and a rule}

Here are some small examples of Hoare triples. In each case the variables
should be understood to be integers%

\[
\{x+1=y\}\;x:=x+1;\;\{x=y\}
\]
Is this triple correct?\ (Answer for yourself before reading on...) If
initially $y$ is $x+1$ and we change $x$ to $x+1$, then finally both $x$ and
$y$ will equal the original value of $x+1$, and so they will equal each other.
Yes, it is correct.

How about%
\[
\{2x=3y\}\;x:=2x;\;\{x=3y\}
\]
Is this correct? Well, if initially $2x$ is $3y$, then, after changing $x$ to
$2x$, finally $x$ will be $3y$.

These two examples suggest a general rule, which is%
\[
\vdash\left\{  Q_{x\leftarrow E}\right\}  \;x:=E;\;\left\{  Q\right\}
\]
When $Q$ is the postcondition of an assignment $x:=E$, we call $Q_{x\leftarrow
E}$ the \textbf{substituted postcondition}. More generally, it is
sufficient\ for the precondition to imply $Q_{x\leftarrow E}$, so a better
rule is%
\begin{equation}
\vdash\left\{  P\right\}  \;v:=E;\;\left\{  Q\right\}  \text{ exactly if
}P\Rightarrow Q_{v\leftarrow E}\text{ is universally true}\nonumber
\end{equation}
where $v$ is any variable, $E$ is any expression, and $P$ and $Q$ are any
conditions. Here is an example where the precondition is stronger than it
needs to be:%
\[
\left\{  2x<3y\right\}  \;x:=2x;\;\left\{  x\leq3y\right\}
\]
The substituted postcondition is $(x\leq3y)_{x\leftarrow2x}$, which is
$2x\leq3y$. By the assignment rule, this triple is correct exactly if
$2x<3y\Rightarrow2x\leq3x$ is universally true.

There is one more aspect to assignment that should be mentioned. This is that
the expression might not always be well defined. For example, if we divide by
$0$, this is an error and we should consider that if this happens the program
has crashed. Since a command that crashes is not partially correct we should
really ensure that our rule for assignments includes checking that the
expression is well defined. Let's suppose that for each expression $E$ there
is a condition $\mathrm{df}[E]$ that says that $E$ is well-defined, i.e. does
not crash when evaluated. For example $\mathrm{df}[x/y]$ might be $y\neq0$.
Now the improved assignment rule is
\begin{equation}%
\begin{tabular}
[c]{ll}%
$\vdash\left\{  P\right\}  \;v:=E;\;\left\{  Q\right\}  $ & $\text{exactly if
}P\Rightarrow Q_{v\leftarrow E}\text{ is universally true}$\\
& $\text{and }P\Rightarrow\mathrm{df}[E]\text{ is universally true}$%
\end{tabular}
\ \ \text{ }\tag{assignment rule}%
\end{equation}
In many cases $\mathrm{df}[E]$ is simply $\mathrm{true}$, and so it is trivial
that $P\Rightarrow\mathrm{df}[E]$ is universally true.

This rule generalizes to simultaneous assignments to multiple variables. For
example%
\[
\left\{  x<y\right\}  \;x,y:=y,x;\;\left\{  y\leq x\right\}
\]
The substituted postcondition is $\left(  y\leq x\right)  _{x,y\leftarrow
y,x}$, which is $x\leq y$; this is implied (for all values of $x$ and $y$) by
$x<y$.

Here is one last example of an assignment; it will be of use later.%
\[
\left\{  y\geq0\wedge x=X\wedge y=Y\right\}  \;z:=1;\;\left\{  y\geq0\wedge
X^{Y}=z\times x^{y}\right\}
\]
First we find the substituted postcondition%
\[
y\geq0\wedge X^{Y}=1\times x^{y}%
\]
which simplifies to
\[
y\geq0\wedge X^{Y}=x^{y}%
\]
This (using one-point laws) is implied by the precondition $y\geq0\wedge
x=X\wedge y=Y$.

\subsection{A bigger example}

Here is another example. I claim that
\begin{equation}
\left\{  y\geq0\wedge x=X\wedge y=Y\right\}  \;S\;\left\{  z=X^{Y}\right\}
\qquad\text{,} \label{a}%
\end{equation}
is correct, where
\begin{align*}
S  &  \triangleq\left(  z:=1;\mathbf{while(}\;y>0\;)\;T\right) \\
T  &  \triangleq\mathbf{if}(\;\mathit{\mathrm{odd}}(y)\;)\;U\;\mathbf{else}%
\;V\\
U  &  \triangleq(z:=z\times x;y:=y-1;)\\
V  &  \triangleq(x:=x\times x;y:=y/2;)\qquad\text{.}%
\end{align*}


To show that this triple is correct, we'll need to deal with constructs other
than assignments. For that we introduce a new idea:\ proof outlines.

\subsection{Proof outlines}%

%TCIMACRO{\TeXButton{B Fig}{\begin{figure}[tb]}}%
%BeginExpansion
\begin{figure}[tb]%
%EndExpansion


\begin{code}
\#\# $y\geq0\wedge x=X\wedge y=Y$

$z:=1$ ;

\#\# $I:y\geq0\wedge X^{Y}=z\times x^{y}$

\textbf{while}(\ $y>0$ )

\#\# $X^{Y}=z\times x^{y}\wedge y>0$

\{

\begin{indent}
\item \textbf{if}(\ $\mathrm{odd}(y)$ )

\item \#\# $X^{Y}=z\times x^{y}\wedge y>0\wedge\mathrm{odd}(y)$

\item \{

\begin{indent}
\item $z:=z\times y$;

\item \#\# $y-1\geq0\wedge X^{Y}=z\times x^{y-1}$

\item $y:=y-1;$
\end{indent}

\item \}

\item \textbf{else}

\item \#\# $X^{Y}=z\times x^{y}\wedge y>0\wedge\mathrm{even}(y)$

\item \{

\begin{indent}
\item $x:=x\times x;$

\item \#\# $\mathrm{even}(y)\wedge y/2\geq0\wedge X^{Y}=z\times x^{y/2}$

\item $y:=y/2;$
\end{indent}

\item \}
\end{indent}

\}

\#\# $z=X^{Y}$
\end{code}

%

%TCIMACRO{\TeXButton{E Fig}{\caption{An example proof outline.}\label
%{po}\end{figure}}}%
%BeginExpansion
\caption{An example proof outline.}\label{po}\end{figure}%
%EndExpansion


A \textbf{proof outline} is a command that is annotated with assertions. It
represents the outline of a proof of the program. Figure \ref{po} is a proof
outline for the example of the last section.

A proof outline is not a proof: it is (if correct)\ a summary of a proof. This
is why it is called a `proof outline'.

\subsection{Correctness of proof outlines}

We can formally define \textbf{partially correct} proof outlines for
sequential programs as follows:

\textbf{Assignment Rule:} $\left\{  P\right\}  \;v:=E;\;\left\{  Q\right\}  $
is a partially correct proof outline if
\[
P\Rightarrow Q_{v\leftarrow E}\text{ is universally true and }P\Rightarrow
\mathrm{df}[E]\text{ is universally true\qquad. }%
\]


\textbf{Skip Rule:} $\left\{  P\right\}  \;\mathbf{skip}\;\left\{  Q\right\}
$ is a partially correct proof outline if%
\[
P\Rightarrow Q\text{ is universally true\qquad.}%
\]


\textbf{(Sequential) Composition Rule:} $\left\{  P\right\}  \;S\;\left\{
Q\right\}  \;T\;\left\{  R\right\}  $ is a partially correct proof outline, provided

\begin{itemize}
\item that $\left\{  P\right\}  \;S\;\left\{  Q\right\}  $ is a partially
correct proof outline, and

\item that $\left\{  Q\right\}  \;T\;\left\{  R\right\}  $ is a partially
correct proof outline.
\end{itemize}

\textbf{2-Tailed If Rule}: $\left\{  P\right\}  \;\mathbf{if(}\;E\;)\;\{Q_{0}%
\}\ S\;\mathbf{else}\;\{Q_{1}\}\;T\;\left\{  R\right\}  $ is a partially
correct proof outline, provided

\begin{itemize}
\item that $\{Q_{0}\}\;S\;\{R\}$ is a partially correct proof outline,

\item that $\{Q_{1}\}\;T\;\{R\}$ is a partially correct proof outline,

\item that $P\Rightarrow\mathrm{df}[E]$ is universally true,

\item that $P\wedge E\Rightarrow Q_{0}$ is universally true, and

\item that $P\wedge\lnot E\Rightarrow Q_{1}$ is universally true.
\end{itemize}

\textbf{1-Tailed If Rule:} $\left\{  P\right\}  \;\mathbf{if(}%
\;E\;)\;\{Q\}\;S\;\left\{  R\right\}  $ is a partially correct proof outline, provided

\begin{itemize}
\item that $\{Q\}\;S\;\{R\}$ is a partially correct proof outline,

\item that $P\Rightarrow\mathrm{df}[E]$ is universally true,

\item that $P\wedge E\Rightarrow Q$ is universally true, and

\item that $P\wedge\lnot E\Rightarrow R$ is universally true.
\end{itemize}

\textbf{Iteration Rule:} $\left\{  P\right\}  \;\mathbf{while(}%
\;E\;)\;\{Q\}\;S\;\left\{  R\right\}  $ is a partially correct proof outline, provided

\begin{itemize}
\item that $P\Rightarrow\mathrm{df}[E]$ is universally true,

\item that $P\wedge E\Rightarrow Q$ is universally true,

\item that $P\wedge\lnot E\Rightarrow R$ is universally true, and

\item that $\{Q\}\;S\;\{P\}$ is a partially correct proof outline.
\end{itemize}

By the way, the loop's precondition, $P$, is called an \textbf{invariant} of
the loop. Loop invariants are crucial in designing and documenting loops. Note
that, provided it is true when the while command starts, the invariant will be
true at the start of each iteration and when the loop terminates. Loop
invariants allow us to analyze the effect of a loop by considering only the
effect of a single iteration.\footnote{Loop invariants are closely related to
global invariants, class or module invariants, and monitor invariants that we
will see later in this course. All can be considered a kind of loop
invariant.}

\textbf{Parentheses Rule:} $\left\{  P\right\}  \;(S)\;\left\{  Q\right\}  $
is a partially correct proof outline, provided $\{P\}\;S\;\{Q\}$ is a
partially correct proof outline.\footnote{Remember that in larger examples, we
use braces instead of parentheses.
\par
In Figure \ref{po} I was careful to place the assertions before the braces at
the start of the loop body and at the start of each tail of the if command.
Throughout the course, I'll place such assertions after the brace at times,
just to save some vertical space. I.e., I'll write%
\[%
\begin{array}
[t]{l}%
\{\;\#\#\;P\\
\quad S\ \}\\
\#\#\ Q
\end{array}
\]
rather than%
\[%
\begin{array}
[t]{l}%
\#\#\;P\\
\{\;S\ \}\\
\#\#\ Q
\end{array}
\]
With the former notation, the \textquotedblleft$\{\;\#\#\;P$\textquotedblright%
\ can often be conveniently placed on the same line as an \textquotedblleft%
$\mathbf{if}(E)$\textquotedblright\ or a \textquotedblleft$\mathbf{while}%
(E)$\textquotedblright}

From here on we will write \textquotedblleft\textbf{correct}\textquotedblright%
\ in place of \textquotedblleft partially correct\textquotedblright, as we
won't be concerned with any other sort of correctness.

In a proof outline, all commands will be preceded by an assertion. This is its
\textbf{precondition}. In the example, the precondition of $y:=y/2;$ is
\[
\mathrm{even}(y)\wedge y/2\geq0\wedge X^{Y}=z\times x^{y/2}%
\]
and the precondition of $x:=x\times x;$ is%
\[
X^{Y}=z\times x^{y}\wedge y>0\wedge\mathrm{even}(y)\text{\qquad.}%
\]


Suppose $\{P\}\;S$\ $\{Q\}$ is a correct proof outline. Let $\widehat{S}$ be
formed by deleting all assertions from $S$ or by treating them as comments.
Now $\{P\}\;\widehat{S}\;\{Q\}$ is a correct Hoare triple.

\subsection{Correctness of the example}

There is a little, but not much, work left to show that the example proof
outline in Figure \ref{po} is correct. First a recall that\ a boolean formula
is said to be universally true if it evaluates to true regardless of the
values chosen for its free variables (including our so-called constants).

Let's call the loop invariant $I$.%
\[
I\triangleq y\geq0\wedge X^{Y}=z\times x^{y}%
\]


\begin{itemize}
\item Because of the first assignment, we must show%
\[
y\geq0\wedge x=X\wedge y=Y\Rightarrow I_{z\leftarrow1}%
\]
is universally true. After substitution we have%
\[
y\geq0\wedge x=X\wedge y=Y\Rightarrow y\geq0\wedge X^{Y}=1\times x^{y}%
\]
which (using a one-point law) we can easily see is universally true.

\item From the rule for while-loops, we must show%
\begin{align*}
&  I\wedge y>0\Rightarrow X^{Y}=z\times x^{y}\wedge y>0\\
&  I\wedge\lnot\left(  y>0\right)  \Rightarrow z=X^{Y}%
\end{align*}
are each universally true. Both are fairly straight-forward

\item From the rule for 2-tailed if commands, we must show%
\begin{align*}
X^{Y}  &  =z\times x^{y}\wedge y>0\wedge\mathrm{odd}(y)\Rightarrow
X^{Y}=z\times x^{y}\wedge y>0\wedge\mathrm{odd}(y)\text{ and}\\
X^{Y}  &  =z\times x^{y}\wedge y>0\wedge\lnot\mathrm{odd}(y)\Rightarrow
X^{Y}=z\times x^{y}\wedge y>0\wedge\mathrm{even}(y)
\end{align*}
are each universally true. Both are trivial.

\item The four assignments in the loop body give rise to four expressions that
should be shown to be universally true:%
\begin{align*}
&  \left.  X^{Y}=z\times x^{y}\wedge y>0\wedge\mathrm{odd}(y)\right.
\Rightarrow\left(  y-1\geq0\wedge X^{Y}=z\times x^{y-1}\right)  _{z\leftarrow
z\times y}\\
&  \left.  y-1\geq0\wedge X^{Y}=z\times x^{y-1}\right.  \Rightarrow
I_{y\leftarrow y-1}\\
&  \left.  X^{Y}=z\times x^{y}\wedge y>0\wedge\mathrm{even}(y)\right.
\Rightarrow\left(  \mathrm{even}(y)\wedge y/2\geq0\wedge X^{Y}=z\times
x^{y/2}\right)  _{x\leftarrow x\times x}\\
&  \left.  \mathrm{even}(y)\wedge y/2\geq0\wedge X^{Y}=z\times x^{y/2}\right.
\Rightarrow I_{y\leftarrow y/2}%
\end{align*}

\end{itemize}

\section{Concurrent programming\label{conc}}

\subsection{Interference}

It was recognized early on that the style of reasoning shown above can be
invalid in the presence of concurrent programs sharing the same variables. For
example, if we have the program\footnote{The angle brackets indicate `atomic
actions', that is commands that are executed without interruption. We will
formalize this notion later.}

\begin{code}
\#\# $\mathit{true}$

$\left\langle x:=1;\right\rangle $

\#\# $x=1$

$\left\langle y:=x;\right\rangle $

\#\# $y>0$
\end{code}

\noindent and we run it concurrently with the program $\left\langle
x:=x+1;\right\rangle $, then the proof outline above is problematic. Consider
what happens if the assignment $x:=x+1;$ is scheduled after the assignment
$x:=1;$ and before the assignment $y:=x;$. Then the precondition of $y:=x;$,
i.e. $x=1$ is not true after the increment happens. In a sense the program
$x:=x+1;$ has \textquotedblleft interfered with\textquotedblright\ the state
of the other program. Worse still, the assignment has interfered with the
proof of the other program, so we can no longer trust our reasoning!

Let's consider the concurrent program above again. Our intuition is that the
concurrent assignment $x:=x+1;$ shouldn't invalidate the conclusion that in
the end $y>0$. One way to validate this intuition is to consider all possible
interleavings\footnote{An interleaving of two sequences of actions is a
sequence that consists of all the actions of the two sequences in the same
relative order. For example if we have two sequences $\left[  a_{0}%
,a_{1}\right]  $ and $\left[  b_{0},b_{1}\right]  $, the possible
interleavings are
\begin{align*}
&  \left[  a_{0},a_{1},b_{0},b_{1}\right]  \text{, }\left[  a_{0},b_{0}%
,a_{1},b_{1}\right]  \text{, }\left[  a_{0},b_{0},b_{1},a_{1}\right]  \text{,
}\\
&  \left[  b_{0},a_{0},a_{1},b_{1}\right]  \text{, }\left[  b_{0},a_{0}%
,b_{1},a_{1}\right]  \text{, and }\left[  b_{0},b_{1},a_{0},a_{1}\right]  .
\end{align*}
} of the assignments --- in this example there are only three possible
interleavings, so this is not much work --- however, we will find that, in
general, it is impractical to consider all possible interleavings and to use
sequential reasoning.

So what can we do? If we use a different proof of the sequential program, namely

\begin{code}
\#\# $\mathit{true}$

$\left\langle x:=1;\right\rangle $

\#\# $x>0$

$\left\langle y:=x;\right\rangle $

\#\# $y>0$
\end{code}

\noindent then the assignment $\left\langle x:=x+1;\right\rangle $ can not
change the condition $x>0$ from true to false. Thus no interleaving of the two
programs will invalidate the proof above. Since we don't know when the
assignment $x:=x+1$ will happen, we should check that the assignment will not
interfere with any of the assertions in the longer program. That is, it will
not cause them, once made true by the first program, to become false. Formally
we should check that%
\begin{align*}
&  \vdash\{\mathit{true}\}\quad\left\langle x:=x+1;\right\rangle \quad\left\{
\mathit{true}\right\}  \qquad\text{,}\\
&  \vdash\{x>0\}\quad\left\langle x:=x+1;\right\rangle \quad\left\{
x>0\right\}  \qquad\text{, and}\\
&  \vdash\{y>0\}\quad\left\langle x:=x+1;\right\rangle \quad\left\{
y>0\right\}  \qquad.
\end{align*}


The insight that Owicki and Gries provided is that, when the proof of a
program is not interfered with by another program, it doesn't matter if the
state is interfered with. This allows us to still use Hoare logic and proof
outlines, provided we are careful, even when dealing with concurrent programs
--- and this is very important because, while we can often trust ourselves to
reason intuitively rather than formally about sequential programs, the same
can not be said for concurrent programs. Proof outlines provide a useful
record of all the assertions that must not be interfered with by actions of
concurrently running commands.

\subsection{Rule for concurrent execution}

We can extend the logic of partial correctness to include a command for
concurrent execution, by which we mean an arbitrary interleaving of the atomic
actions of two processes.\footnote{This may seem an odd definiton of
concurrent, since it doesn't suggest that two actions might happen at the same
time. However, if action $a_{i}$ has started already then action $b_{j}$ can
be started if it is independant of $a_{i}$.} We'll use the notation%
\[
\mathbf{co}\;S\;//\;T\;\mathbf{oc}%
\]
for the concurrent composition of two commands $S$ and $T$.

Suppose $\{P_{S}\}S\{Q_{S}\}$ and $\{P_{T}\}T\{Q_{T}\}$ are two proof
outlines. Suppose that $a$ is an atomic action from $\{P_{S}\}S\{Q_{S}\}$ with
a precondition of $R$, and $P$ is an assertion from $\{P_{T}\}T\{Q_{T}\}$
(possibly $P_{T}$ or $Q_{T}$\textbf{,} but also possibly an assertion embedded
within command $T$), then $a$ \textbf{does not interfere with} $P$ if%
\[
\vdash\{P\wedge R\}\quad a\quad\{P\}
\]
Furthermore the two proof outlines \textbf{do not interfere with each other}
if no action of one interferes with any assertion in the
other.\footnote{Later, when we get to await commands, we'll have to modify
this definition a little.}

\textbf{Concurrent execution Rule:}
\[
\{P\}\;\mathbf{co}\;\{P_{S}\}S\{Q_{S}\}\;//\;\{P_{T}\}T\{Q_{T}\}\;\mathbf{oc}%
\;\{Q\}
\]
is a (partially) correct proof outline iff

\begin{itemize}
\item the precondition of the co command implies the precondition of each
component, i.e.,%
\begin{align*}
P  &  \Rightarrow P_{S}\text{ is universally true and}\\
P  &  \Rightarrow P_{T}\text{ is universally true,}%
\end{align*}


\item the conjunction of the postconditions of its components implies the
postcondition of the co command, i.e.,%
\[
Q_{S}\wedge Q_{T}\Rightarrow Q\text{ is universally true,}%
\]


\item $\{P_{S}\}S\{Q_{S}\}$ and $\{P_{T}\}T\{Q_{T}\}$ are both correct proof
outlines, and

\item $\{P_{S}\}S\{Q_{S}\}$ and $\{P_{T}\}T\{Q_{T}\}$ do not interfere with
each other (freedom from interference).
%
%Is this enough? Should I also insist that all assignments are atomic. Also what
%about if conditions and while conditions, should they be atomic?

\end{itemize}

\subsection{Awaiting}

Processes can synchronize by means of an await command%
\[
\left\langle \mathbf{await}(\;E\;)\;S\right\rangle
\]
where $E$ is a boolean expression and $S$ is any command\footnote{Except that
await commands are not allowed inside of await commands, nor are co
commands.}. The idea is that the process delays until $E$ becomes true and
then the command $S$ is executed atomically with the evaluation of $E$.

\textbf{Await rule:\ }A proof outline%
\[
\{P\}\;\left\langle \mathbf{await}(\;E\;)\;S\right\rangle \;\{R\}
\]
is correct iff $\{P\wedge E\}\;S\;\{R\}$ is a correct proof outline.

Thus, in a sense, the await command causes $E$ to become true, almost as if by
magic. For example the following correct proof outline appears to set $x$ to
$99$ without doing any real work

\begin{code}
\#\# true

\textbf{co}

\begin{indent}
\item \#\#\ true

\item $\left\langle \mathbf{await}(\;x=99\;)\;\mathbf{skip};\right\rangle $

\item \#\# $x=99$
\end{indent}

//

\begin{indent}
\item \#\#\ true

\item \textbf{skip}

\item \#\#\ true
\end{indent}

\textbf{oc}

\#\# $x=99$
\end{code}

\noindent Of course there is no magic about it; if $E$ is not true when the
await command starts to delay, then it is up to the other process to
eventually make $E$ true; if that doesn't happen, the await will delay the
process forever. Remember that we are dealing only with partial correctness
here, so a correct proof outline may still embody a program that will never terminate.

As abbreviations we have:

\begin{itemize}
\item $\left\langle S\right\rangle $ abbreviates $\,\left\langle
\mathbf{await}(\;\mathit{true}\;)\;S\right\rangle $, and

\item $\left\langle \mathbf{await}(\;E\;)\right\rangle $ abbreviates
$\left\langle \mathbf{await}(\;E\;)\;\mathbf{skip}\right\rangle $
\end{itemize}

The derived rules are that $\{P\}\;\left\langle S\right\rangle \;\{R\}$ is a
correct proof outline if $\{P\}\;S\;\{R\}$ is a correct proof outline, and
that $\{P\}\;\left\langle \mathbf{await}(\;E\;)\right\rangle \;\{R\}$ is a
correct proof outline if
\[
P\wedge E\Rightarrow R\text{ is universally true.}%
\]


To account for the atomicity of the await commands, we modify the definition
of `proof outlines do not interfere with each other'\ to exclude assertions
that are contained within await commands. I.e. two proof outlines \textbf{do
not interfere with each other} exactly if no action of one interferes with any
assertion in the other not contained in an await command.

\section{A formalization of proof-outline logic\label{formalization}}

The next two subsections are optional reading.\ I\ wrote them mainly to
clarify my own understanding. I\ include them as they may also help clarify
yours.\ If you skip them, proceed to the important caveat in subsection
\ref{sec:caveat}.

\subsection{Syntax}

Let's take $V$ to be a set of variables, $T$ to be set of types, $E$ to be a
set of expressions, and $P$ to be a set of conditions. The syntax for
assignments $A$, commands $S$, blocks $B$ and proof outlines $O$ is given by:%

\[%
\begin{array}
[t]{llll}%
A & \rightarrow & V:=E; & \text{Assignment}\\
A & \rightarrow & V,A,E & \text{Multiple assignment}\\
S & \rightarrow & A; & \text{Assignment command}\\
S & \rightarrow & \mathbf{skip} & \text{Skip command}\\
S & \rightarrow & (B) & \text{Block command}\\
S & \rightarrow & \mathbf{if}(\;E\;)\;\{P\}\;S\;\mathbf{else}\;\{P\}\;S &
\text{2-tailed if command}\\
S & \rightarrow & \mathbf{if}(\;E\;)\;\{P\}\;S & \text{1-tailed if command}\\
S & \rightarrow & \mathbf{while}(\;E\;)\;\{P\}\;S & \text{While command}\\
S & \rightarrow & \mathbf{co}%
\;\{P\}\;B\;\{P\}\;//\;\{P\}\;B\;\{P\}\;\mathbf{oc} & \text{Concurrent
command}\\
S & \rightarrow & \left\langle \mathbf{await}(\;E\;)\;B\right\rangle  &
\text{Await command}\\
B & \rightarrow & T\;V\;;\;B & \text{Variable declaration}\\
B & \rightarrow & S\;\{P\}\;B & \text{Sequential composition}\\
B & \rightarrow & S & \text{Simple block}\\
O & \rightarrow & \left\{  P\right\}  \;B\;\left\{  P\right\}  & \text{Proof
outline}%
\end{array}
\]


There are a number of restrictions not indicated by the syntax

\begin{itemize}
\item In an assignment command, the type of the $i^{\text{th}}$ variable must
match the type of the $i^{\text{th}}$ expression, for each $i$.

\item The expressions in await, if, and while commands must be boolean.

\item The scope of a variable is the block that follows its declaration.

\item Await commands may not occur within await commands, directly or indirectly.

\item The choice between 1- and 2-tailed if commands leads to a syntactic
ambiguity. As we read from left to right, each `else' is considered attached
to the nearest as-yet-umatched `if' to its left to which it could be attached.
For example
\[
\mathbf{if}(E_{0})\;\left\{  P_{0}\right\}  \;\mathbf{if}(E_{1})\;\left\{
P_{1}\right\}  \;S_{0}\;\mathbf{else}\;\left\{  P_{2}\right\}  \;S_{1}%
\]
is treated as a 2-tailed if command within a 1-tailed if command.
\end{itemize}

A few other comments are in order.

\begin{itemize}
\item I only include the binary case for the concurrent command. The
generalization to more processes is straight-forward.

\item I\ omit the abbreviations for await commands. These are $\left\langle
B\right\rangle $ abbreviates $\left\langle \mathbf{await}(\;\mathit{true}%
\;)\;B\right\rangle $ and $\left\langle \mathbf{await}(\;P\;)\right\rangle $
abbreviates $\left\langle \mathbf{await}(\;P\;)\;\mathbf{skip}\right\rangle $.

\item I use round parentheses to turn blocks into commands rather than the
curly braces used by Andrews and in my examples. This is so that the curly
braces can be saved to delimit assertions. See next point.

\item In this section I use $\left\{  P\right\}  $ to indicate an assertion,
rather than the $\#\#P$ notation used in other sections and in Andrew's text.

\item In practice a fragment \textquotedblleft$\left\{  P\right\}
($\textquotedblright\ (or \textquotedblleft$%
\begin{array}
[c]{l}%
{\small \#\#P}\\
{\small \{}%
\end{array}
$\textquotedblright) may be written as \textquotedblleft$(\left\{  P\right\}
$\textquotedblright\ (or \textquotedblleft$\{\,\#\#P$\textquotedblright) . In
Andrews' notation, the latter often formats a bit better. In dealing with the
theory, the former is easier to cope with.

\item I\ haven't gone in to detail about the set of expressions. I will assume
though that, for each expression $E$, there is a condition $\mathrm{df}[E]$
that expresses the weakest condition for $E$ to be well-defined, i.e. not to
crash. For example if dividing by $0$ is considered to be a cause for crashing
then $\mathrm{df}[x/y]$ would be $y\neq0$. If $a$ is an array of size $n$ then
$\mathrm{df}[a[i]]$ would be $0\leq i<n$.
\end{itemize}

\subsection{Semantics}

\subsubsection{Underlying logic}

We'll assume there is an underlying logic with judgements of the form $\vdash
P$ where $P$ is a predicate logic formula. So, for example,%
\[
\vdash x+y=y+x
\]
means that \textquotedblleft$x+y=y+x$\textquotedblright\ is a theorem in the
underlying logic, i.e. that it is universally true and can be proved so in the
underlying logic.

\subsubsection{Rules}

We put a turnstile ($\vdash$) in front of a proof outline to indicate that it
is (partially) correct.

The following inference rules allow us to conclude that certain proof outlines
are correct. Each inference rule%
\[
\frac{%
\begin{array}
[c]{c}%
A_{0}\\
A_{1}\\
...\\
A_{n}%
\end{array}
}{C}%
\]
is interpreted as follows:\ If the list of antecedents $A$ on the top of the
line is true, then the consequent $C$, below the line is true.\footnote{Those
who have studied Hoare's logic will note the absence of the rule(s) of
consequence. Consequence is incorporated as needed in the various rules.}%
\[
\frac{%
\begin{array}
[c]{c}%
\vdash P\Rightarrow\mathrm{df}[e_{0}]\wedge\mathrm{df}[e_{1}]\wedge
\cdots\wedge\mathrm{df}[e_{n-1}]\\
\vdash P\Rightarrow Q_{v_{0},v_{1},...,v_{n-1}\leftarrow e_{0},e_{1}%
,...,e_{n-1}}%
\end{array}
}{\vdash\{P\}\;v_{0},v_{1},...,v_{n-1}:=e_{0},e_{1},...,e_{n-1};\;\{Q\}}%
\text{(Assign)}%
\]%
\[
\frac{\vdash P\Rightarrow Q}{\vdash\{P\}\;\mathbf{skip}\;\{Q\}}%
\text{(Skip)\quad}\frac{%
\begin{array}
[c]{c}%
\vdash P\Rightarrow\mathrm{df}[E]\\
\vdash\{P\wedge E\}\;B\;\{Q\}
\end{array}
}{\vdash\{P\}\;\left\langle \mathbf{await}(\;E\;)\;B\right\rangle
\;\{Q\}}\text{(Await)}%
\]%
\[
\frac{%
\begin{array}
[c]{l}%
\vdash\{Q_{0}\}\;S_{0}\;\{R\}\\
\vdash\{Q_{1}\}\;S_{1}\;\{R\}\\
\vdash P\Rightarrow\mathrm{df}[E]\\
\vdash P\wedge E\Rightarrow Q_{0}\\
\vdash P\wedge\lnot E\Rightarrow Q_{1}%
\end{array}
}{\vdash\{P\}\;\mathbf{if}(\;E\;)\;\{Q_{0}\}\;S_{0}\;\mathbf{else}%
\;\{Q_{0}\}\;S_{1}\;\{R\}}\text{(If 2)\quad}\frac{%
\begin{array}
[c]{l}%
\vdash\{Q\}\;S\;\{R\}\\
\vdash P\Rightarrow\mathrm{df}[E]\\
\vdash P\wedge E\Rightarrow Q\\
\vdash P\wedge\lnot E\Rightarrow R
\end{array}
}{\vdash\{P\}\;\mathbf{if}(\;E\;)\;\{Q\}\;S\;\{R\}}\text{(If 1)}%
\]%
\[
\frac{%
\begin{array}
[c]{l}%
\vdash\{Q\}\;S\;\{P\}\\
\vdash P\Rightarrow\mathrm{df}[E]\\
\vdash P\wedge E\Rightarrow Q\\
\vdash P\wedge\lnot E\Rightarrow R
\end{array}
}{\vdash\{P\}\;\mathbf{while}(\;E\;)\;\{Q\}\;S\;\{R\}}\text{(While)\quad}%
\frac{%
\begin{array}
[c]{l}%
\vdash\{P\}\;S\;\{Q\}\\
\vdash\{Q\}\;B\;\{R\}
\end{array}
}{\vdash\{P\}\;S\;\{Q\}\;B\;\{R\}}\text{(Seq)}%
\]%
\[
\frac{%
\begin{array}
[c]{l}%
\vdash\{P_{0}\}\;B_{0}\;\{Q_{0}\}\\
\vdash\{P_{1}\}\;B_{1}\;\{Q_{1}\}\\
\vdash P\Rightarrow P_{0}\\
\vdash P\Rightarrow P_{1}\\
\vdash Q_{0}\wedge Q_{1}\Rightarrow Q\\
\{P_{0}\}\;B_{0}\;\{Q_{0}\}\text{ does not interfere with }\{P_{1}%
\}\;B_{1}\;\{Q_{1}\}
\end{array}
}{\vdash\{P\}\;\mathbf{co}\;\{P_{0}\}\;B_{0}\;\{Q_{0}\}\;//\;\{P_{1}%
\}\;B_{1}\;\{Q_{1}\}\;\mathbf{oc}\;\{Q\}}\text{(Co)}%
\]%
\[
\frac{%
\begin{array}
[c]{l}%
\vdash\{P\}\;B\;\{Q\}\\
v\text{ is not free in }P\\
v\text{ is not free in }Q
\end{array}
}{\vdash\left\{  P\right\}  \;T\;v\;;\;B\;\{Q\}}\text{(Decl)\quad}\frac
{\vdash\{P\}\;B\;\{Q\}}{\vdash\{P\}\;(B)\;\{Q\}}\text{(Paren)}%
\]


\subsection{A caveat\label{sec:caveat}}

These rules assume a certain granularity of execution that may not be
realistic. You can see that $x:=E;$ and $\left\langle x:=E;\right\rangle $
mean exactly the same thing, so we are assuming that assignment commands are
executed atomically. Similarly the guard expressions in if and while commands
are assumed to be evaluated atomically. These assumptions are not actually
realistic. Languages such as C, C++, and Java have carefully defined rules
defining the meaning of reads from and writes to shared variables; these are
the so-called `memory models' for the languages.

\section{Simple examples\label{simple}}

\subsection{A note on showing triples involving assignments}

To show noninterference, one has to show that atomic actions do not interfere
with assertions. Typically the atomic actions are assignments, so let's look a
bit at showing Hoare triples that involve assignments to be correct.

Generally we need to show%
\[
\{P\}\;\left\langle x:=E;\right\rangle \;\{Q\}
\]
is correct. Some observations:

\begin{itemize}
\item The Hoare-triple is correct if and only if%
\[
P\Rightarrow Q_{x\leftarrow E}%
\]
is universally true. We call $Q_{x\leftarrow E}$ \textbf{the substituted
postcondition}. So we must show that the precondition implies the substituted postcondition.

\item If $P$ is universally false,\footnote{I.e. $\lnot P$ is universally
true.} then the Hoare triple is correct. This follows immediately from the
previous point. Usually when a precondition is universally false, it is
because you have come across some form of \textbf{mutual exclusion}.

\item If the precondition is a conjunction, for example $P$ is $\left(
P0\wedge P1\wedge P2\right)  $, it is safe to use only some of the conjuncts
of a precondition. For example, it would be sufficient to show%
\[
\vdash P0\Rightarrow Q_{x\leftarrow E}\qquad\text{.}%
\]
We\ call this \textbf{subsetting the precondition}.

\item Extending this a bit further, it is safe to replace the precondition $P$
by any condition $R$ that is implied by the $P$. At the extreme (taking $R$ as
$\mathbf{true}$), it suffices to ignore the precondition altogether and simply
show $Q_{x\leftarrow E}$ is universally true.

\item We can show the postcondition in parts. For example if $Q$ is $Q0\wedge
Q1$ then we can separately show%
\begin{align*}
&  \vdash P\Rightarrow Q0_{x\leftarrow E}\qquad\text{, and}\\
&  \vdash P\Rightarrow Q1_{x\leftarrow E}%
\end{align*}
are correct. We call this \textbf{proof by parts}.
\end{itemize}

When showing non-interference, the postcondition is also part of the
precondition. We need to show%
\[
\vdash\{Q\wedge P\}\;\left\langle x:=E;\right\rangle \;\{Q\}
\]
where $Q$ is some assertion made in one component, $\left\langle
x:=E;\right\rangle $ is an atomic action from another component, and $P$ is
the precondition of $\left\langle x:=E;\right\rangle $.

\begin{itemize}
\item If $x$ does not occur in $Q$, then the Hoare triple is correct. This is
because $Q_{x\leftarrow E}$ is then simply $Q$ and we have to show%
\[
Q\wedge P\Rightarrow Q
\]
universally true, which it trivially is. We call this situation
\textbf{disjoint variables}.
\end{itemize}

The idea of \textbf{proof by parts} applies to outlines in general, not just
to assignment commands. If we want to show%
\[
\left\{  P\right\}  \;S\;\left\{  Q0\wedge Q1\right\}
\]
to be correct, it suffices to show that both%
\begin{align*}
&  \left\{  P\right\}  \;S\;\left\{  Q0\right\}  \text{ and}\\
&  \left\{  P\right\}  \;S\;\left\{  Q1\right\}
\end{align*}
are correct.

\subsection{An example with no interference}

Consider the following program that increments $x$ and $y$ in parallel

\begin{code}
\#\# $x=X\wedge y=Y$

\textbf{co}

\begin{indent}
\item \#\# $x=X$

\item $\left\langle x:=x+1;\right\rangle $

\item \#\# $x=X+1$
\end{indent}

//

\begin{indent}
\item \#\# $y=Y$

\item $\left\langle y:=y+1;\right\rangle $

\item \#\# $y=Y+1$
\end{indent}

\textbf{oc}

\#\# $x=X+1\wedge y=Y+1$
\end{code}

To show this proof outline correct we must check the following

\begin{itemize}
\item \emph{The precondition of the co command implies the preconditions of
each component of the co command.}

\begin{itemize}
\item $x=X\wedge y=Y\Rightarrow x=X$

This is universally true from propositional calculus.

\item $x=X\wedge y=Y\Rightarrow y=Y$

Similarly.
\end{itemize}

\item \emph{The conjunction of the postconditions of its components implies
the postcondition of the co command}$.$

The postconditions of the components are respectively $x=X+1$ and $y=Y+1$. The
conjunction of them is the postcondition of the co command itself.

\item \emph{Local correctness. }We need to show that each component is itself
a correct proof outline. By application of the assignment rule. We have that%
\[
\{x=X\}\;x:=x+1;\;\{x=X+1\}
\]
is correct. Similarly for%
\[
\{y=Y\}\;y:=y+1;\;\{y=Y+1\}
\]


\item \emph{Freedom from interference.} We can consider each pair consisting
of an assertion in one component and an atomic action in the other. There are
four such pairs%
\[%
\begin{tabular}
[c]{|l|l|}\hline
$\{x=X\}$ & $\left\langle y:=y+1;\right\rangle $\\\hline
$\{x=X+1\}$ & $\left\langle y:=y+1;\right\rangle $\\\hline
$\{y=Y\}$ & $\left\langle x:=x+1;\right\rangle $\\\hline
$\{y=Y+1\}$ & $\left\langle x:=x+1;\right\rangle $\\\hline
\end{tabular}
\ \ \
\]
To check the first we must check that%
\[
\{x=X\wedge y=Y\}\;\left\langle y:=y+1;\right\rangle \;\{x=X\}
\]
is correct. From the assignment rule, we must show%
\[
x=X\wedge y=Y\Rightarrow\left(  x=X\right)  _{y\leftarrow y+1}%
\]
to be universally true; it simplifies to
\[
x=X\wedge y=Y\Rightarrow x=X
\]
which is universally true by propositional calculus. In the terminology above,
we have \textquotedblleft disjoint variables\textquotedblright.

The other three action/assertion pairs are each free of interference by reason
of disjoint variables.
\end{itemize}

\subsection{An example with interference}

Consider the following proof outline

\begin{code}
\#\# $x=X$

\textbf{co}

\begin{indent}
\item \#\# $x=X$

\item $\left\langle x:=x+2;\right\rangle $

\item \#\# $x=x+2$
\end{indent}

//

\begin{indent}
\item \#\# $x=X$

\item $\left\langle x:=x+3;\right\rangle $

\item \#\# $x=X+3$
\end{indent}

\textbf{oc}

\#\# $x=X+2\wedge x=X+3$
\end{code}

This has the rather startling postcondition that $x$ is both $X+2$ and that it
is $X+3$ !

What is wrong? Let's check everything

\begin{itemize}
\item \emph{The precondition of the co command implies the precondition of
each component.} As they are the same, the implication is trivial.

\item \emph{The conjunction of the postconditions of its components implies
the postcondition of the co command.} As they are the same, this implication
is also trivial.

\item \emph{Local correctness.} Each component is a correct proof outline.

\item \emph{Freedom from interference.} There are four assertion/action pairs
to check%
\[%
\begin{tabular}
[c]{|l|l|}\hline
$\{x=X\}$ & $\left\langle x:=x+3;\right\rangle $\\\hline
$\{x=x+2\}$ & $\left\langle x:=x+3;\right\rangle $\\\hline
$\{x=X\}$ & $\left\langle x:=x+2;\right\rangle $\\\hline
$\{x=X+3\}$ & $\left\langle x:=x+2;\right\rangle $\\\hline
\end{tabular}
\ \ \
\]
For the first we must show that%
\[
\{x=X\}\;\left\langle x:=x+3;\right\rangle \;\{x=X\}
\]
is correct. But it is not. Thus there is interference. In fact every pair
exhibits interference.
\end{itemize}

\subsection{Fixing the last example.}

Looking at the code for the last example, we would expect the postcondition to
be $x=X+5$. Let's see how we can prove that. We'll reason operationally:
Initially $x=X$ is true, but while the first thread is waiting to execute its
command, the command from the second thread may execute, changing $x$ to
$X+3$. Thus either $x=X$ or $x=X+3$ could be true. Similarly, before the
command in the second component is waiting to execute, the state could be
either $x=X$ or $x=X+2$ (at least). Let's try these as preconditions; we get

\begin{code}
\#\# $x=X$

\textbf{co}

\begin{indent}
\item \#\# $x=X\vee x=X+3$

\item $\left\langle x:=x+2;\right\rangle $

\item \#\# $?$
\end{indent}

//

\begin{indent}
\item \#\# $x=X\vee x=X+2$

\item $\left\langle x:=x+3;\right\rangle $

\item \#\# $?$
\end{indent}

\textbf{oc}

\#\# $x=X+5$
\end{code}

From the preconditions and the assignments, we can see that after the first
command, the state could be either $x=X+2$ or $x=X+5$ and after the second the
state could be either $x=X+3$ or $x=X+5$. So we can complete the proof outline:

\begin{code}
\#\# $x=X$

\textbf{co}

\begin{indent}
\item \#\# $x=X\vee x=X+3$

\item $\left\langle x:=x+2;\right\rangle $

\item \#\# $x=X+2\vee x=X+5$
\end{indent}

//

\begin{indent}
\item \#\# $x=X\vee x=X+2$

\item $\left\langle x:=x+3;\right\rangle $

\item \#\# $x=X+3\vee x=X+5$
\end{indent}

\textbf{oc}

\#\# $x=X+5$
\end{code}

Now is this proof outline correct? We must check:

\begin{itemize}
\item \emph{The precondition of the co command implies the precondition of
each component.} These are true by propositional reasoning as%
\[
P\Rightarrow P\vee Q
\]


\item \emph{The conjunction of the postconditions of the components implies
the postcondition of the co command.} We must check whether%
\[
\left(  x=X+2\vee x=X+5\right)  \wedge\left(  x=X+3\vee x=X+5\right)
\Rightarrow x=X+5
\]
is universally true. We have%
\begin{align*}
&  \left(  x=X+2\vee x=X+5\right)  \wedge\left(  x=X+3\vee x=X+5\right) \\
=  &  \text{Distributivity}\\
&  \left(  x=X+2\wedge x=X+3\right)  \vee\left(  x=X+2\wedge x=X+5\right) \\
&  \vee\left(  x=X+5\wedge x=X+3\right)  \vee\left(  x=X+5\wedge x=X+5\right)
\\
=  &  \text{Simplification}\\
&  \mathrm{false}\vee\mathrm{false}\vee\mathrm{false}\vee\left.  x=X+5\right.
\\
=  &  \text{Identity}\\
&  \left.  x=X+5\right.
\end{align*}


\item \emph{Local correctness.} We need to check the correctness of each
component.\ I.e. the correctness of%
\begin{align*}
&  \left\{  x=X\vee x=X+3\right\}  \;\left\langle x:=x+2;\right\rangle
\;\left\{  x=X+2\vee x=X+5\right\}  \qquad\text{, and}\\
&  \left\{  x=X\vee x=X+2\right\}  \;\left\langle x:=x+3;\right\rangle
\;\left\{  x=X+3\vee x=X+5\right\}  \qquad\text{.}%
\end{align*}
For the first we substitute in the postcondition to get%
\[
x+2=X+2\vee x+2=X+5
\]
which after a bit of algebraic simplification, is equivalent to the
precondition $x=X\vee x=X+3$. Similarly for the second after substitution we
get%
\[
x+3=X+3\vee x+3=X+5\qquad\text{,}%
\]
which simplifies to the precondition.

\item \emph{Freedom from interference.} There are four assertion/action pairs
to check%
\[%
\begin{tabular}
[c]{|l|l|}\hline
$\{x=X\vee x=X+3\}$ & $\left\langle x:=x+3;\right\rangle $\\\hline
$\{x=X+2\vee x=X+5\}$ & $\left\langle x:=x+3;\right\rangle $\\\hline
$\{x=X\vee x=X+2\}$ & $\left\langle x:=x+2;\right\rangle $\\\hline
$\{x=X+3\vee x=X+5\}$ & $\left\langle x:=x+2;\right\rangle $\\\hline
\end{tabular}
\
\]


\begin{itemize}
\item For the first we need to check the correctness of%
\begin{align*}
&  \left\{  \left(  x=X\vee x=X+3\right)  \wedge\left(  x=X\vee x=X+2\right)
\right\} \\
&  \left\langle x:=x+3;\right\rangle \\
&  \left\{  x=X\vee x=X+3\right\}
\end{align*}
The precondition simplifies to $x=X$. The substituted postcondition is
$x+3=X\vee x+3=X+3$. So we need to check that%
\[
x=X\Rightarrow x+3=X\vee x+3=X+3
\]
is universally true, which, by a one-point law, it is.

\item For the second, we need to check%
\begin{align*}
&  \left\{  \left(  x=X+2\vee x=X+5\right)  \wedge\left(  x=X\vee
x=X+2\right)  \right\} \\
&  \left\langle x:=x+3;\right\rangle \\
&  \left\{  x=X+2\vee x=X+5\right\}
\end{align*}
The precondition simplifies to $x=X+2$. The substituted postcondition is
$x+3=X+2\vee x+3=X+5$ which simplifies to $x=X-1\vee x=X+2$. So we need to
check%
\[
x=X+2\Rightarrow x=X-1\vee x=X+2
\]


\item The last two are essentially the same as the first two.
\end{itemize}
\end{itemize}

\section{Global invariants\label{global}}

\subsection{Global invariants}

The purpose of the next example is to illustrate the important technique of
using \textquotedblleft global invariants\textquotedblright. A condition $G$
is a \textbf{global invariant} with respect to a co command if

\begin{itemize}
\item it is implied by the precondition of the co command,

\item each atomic action in the co command preserves the invariant in the
sense that%
\[
\{G\wedge P_{a}\}\;a\;\{G\}
\]
is correct for each atomic action $a$ in the co command, where $P_{a}$ is the
precondition of $a$.
\end{itemize}

Once we've shown a condition to be a global invariant, we can assume it as a
precondition in showing the local correctness of any assertion and when
showing interference freedom. Each assertion $P$ can be split into the global
part and a local part:%
\[
P=G\wedge P_{L}\qquad\text{,}%
\]
where $G$ is the conjunction of all global invariants. For local correctness
we need to show outlines of the form%
\[
\{G\wedge P_{L}\}\;a\;\{G\wedge Q_{L}\}
\]
correct, where $P_{L}$ is the local part of the precondition and $Q_{L}$ is
the local part of the postcondition. Since we've already shown $G$ is a global
invariant, all that remains is to show (using \textquotedblleft proof by
parts\textquotedblright)
\[
\{G\wedge P_{L}\}\;a\;\{Q_{L}\}
\]
correct. For freedom from interference, we need to show outlines of the form%
\[
\{Q_{L}\wedge G\wedge P_{L}\}\;a\;\{G\wedge Q_{L}\}
\]
to be correct, where $P_{L}$ is the local part of the precondition of an
action $a$ and $Q_{L}$ is the local part of some assertion from another
component. If we've already shown that $G$ is a global invariant, it only
remains to show (again, using \textquotedblleft proof by
parts\textquotedblright) that%
\[
\{Q_{L}\wedge G\wedge P_{L}\}\;a\;\{Q_{L}\}
\]
is correct.

We can save a lot of writing by noting the relevant global invariants between
the co command and its precondition. For example,

\begin{code}
\#\# $P$

\#\#\ Global Inv: $G$

\textbf{co}

\begin{indent}
\item \#\# $P_{0}$

\item $\left\langle S\right\rangle $

\item \#\# $Q_{0}$
\end{indent}

//

\begin{indent}
\item \#\# $P_{1}$

\item $\left\langle T\right\rangle $

\item \#\# $Q_{1}$
\end{indent}

\textbf{oc}

\#\# $Q$
\end{code}

\noindent abbreviates

\begin{code}
\#\# $P$

\textbf{co}

\begin{indent}
\item \#\# $G\wedge P_{0}$

\item $\,\left\langle S\right\rangle $

\item \#\# $G\wedge Q_{0}$
\end{indent}

//

\begin{indent}
\item \#\# $G\wedge P_{1}$

\item $\left\langle T\right\rangle $

\item \#\# $G\wedge Q_{1}$
\end{indent}

\textbf{oc}

\#\# $Q$
\end{code}

The next section illustrates the use of global invariants, of await commands
for coordinating cooperating processes, and of mutual exclusion.

\subsection{A client and a server}

In this example, we analyze a shared-variable client-server system. The client
sends a message in shared variable $x$. The server computes a function, $f$,
of $x$ and returns it via the same variable. (Maybe computing $f$ requires
some information that the server has and the client doesn't.) Shared variable
$q$ keeps track of the state of the computation as follows:\ $q=0$ means it's
the clients turn to write to $x$, $q=1$ means it's the server's turn to write
to $x$, $q=2$ means the client is finished and the server may shut-down. The
server looks like this

\begin{code}
\textbf{while}( $q\neq2$ ) \{

\begin{indent}
\item $\left\langle \mathbf{await\;}(q=1)\right\rangle $

\item $\left\langle x,q:=f(x),0;\right\rangle $ \}
\end{indent}
\end{code}

The client computes a function $g(x)$ and sends the result to the server,
getting back the result $f(g(x))$. The client ends after $N$ iterations. The
client looks like this

\begin{code}
\textbf{int} $i:=0$ ;

\textbf{while}( $i<N$ ) \{

\begin{indent}
\item $\left\langle x,q,i:=g(x),1,i+1;\right\rangle $

\item $\left\langle \mathbf{await\;}(q=0)\right\rangle $ \}
\end{indent}

$\left\langle q:=2;\right\rangle $
\end{code}

Initially we will have $x=X$ and $q=0$. Let $h=(f\circ g)$ be the
composition\footnote{That $h$ is the composition $f\circ g$, means
$h(x)=f(g(x))$, for all $x$. We can compose a function with itself, with a
shorthand notation $h^{2}=\left(  h\circ h\right)  $. If $i$ is a natural
number, $h^{i}$ means $i$ copies of $h$ composed together; i.e., $h^{0}(x)=x$,
$h^{1}(x)=h(x)=f(g(x))$, $h^{2}(x)=h(h(x))=f(g(f(g(x))))$, and so on. } of $g$
and $f$. Then the parallel composition of the client and server should compute
$x=h^{N}(X)$. We'll assume all functions are defined for the values they are
applied to.%

%TCIMACRO{\TeXButton{B Fig}{\begin{figure}[tb!]}}%
%BeginExpansion
\begin{figure}[tb!]%
%EndExpansion


\begin{code}
\#\# $x=X$

\textbf{int} $q:=0$ , $i:=0$

\#\# $x=X\wedge q=0\wedge i=0$

\#\#\ Global\ Inv:\ $0\leq q\leq2$

\#\#\ Global Inv: $i\geq0$

\#\#$\ $Global Inv:$\ q\neq1\Rightarrow x=h^{i}(X)$

\#\# Global Inv:$\ q=1\Rightarrow x=g(h^{i-1}(X))\wedge i>0$

\textbf{co}

\begin{indent}
\item \textbf{while}( $q\neq2$ ) \{

\begin{indent}
\item $\left\langle \mathbf{await\;}(q=1)\right\rangle $

\item \#\# $q=1$

\item $\left\langle x,q:=f(x),0;\right\rangle $ \}
\end{indent}
\end{indent}

//

\begin{indent}
\item \#\# $i\leq N\wedge q=0$

\item \textbf{while}( $i<N$ ) \{

\begin{indent}
\item \#\# $i<N\wedge q=0$

\item $\left\langle x,q,i:=g(x),1,i+1;\right\rangle $

\item \#\# $i\leq N$

\item $\left\langle \mathbf{await\;}(q=0)\right\rangle $
\end{indent}

\item \#\# $q=0\wedge i=N$

\item $\left\langle q:=2;\right\rangle $

\item \#\# $q=2\wedge i=N$
\end{indent}

\textbf{oc}

\#\# $x=h^{N}(X)$
\end{code}

%

%TCIMACRO{\TeXButton{E Fig}{\caption{A client and server system.}\label
%{cs}\end{figure}}}%
%BeginExpansion
\caption{A client and server system.}\label{cs}\end{figure}%
%EndExpansion


We need to come up with a proof outline for the composition. \emph{It is a
good idea to keep the annotation as minimal as possible, that is, to resist
the temptation to write down everything we might be able to prove about the
state at each point. Rather we only make note of those facts required to
achieve our goal:\ namely, to have a correct proof outline with the stated
pre- and postconditions..}

Figure \ref{cs} on page \pageref{cs} shows the composition.

In the proof outline in Figure \ref{cs}, I've used the abbreviation mentioned
at the end of the previous section. Rather than repeating the global
invariants in nine different places, I've listed them just once at the start
of the co command. These invariants are to be thought of as being conjoined
with each assertion within the co command. Thus the full precondition of the
command $\left\langle x,q:=f(x),0;\right\rangle $ is%
\[
q=1\wedge0\leq q\leq2\wedge i\geq0\wedge\left(  q\neq1\Rightarrow
x=h^{i}(X)\right)  \wedge\left(  q=1\Rightarrow x=g(h^{i-1}(X))\right)
\]
which simplifies to%
\[
q=1\wedge i\geq0\wedge x=g(h^{i-1}(X))
\]
Furthermore, I\ haven't written assertions that are simply \textit{true.} So
the precondition of $\left\langle \mathbf{await\;}(q=1)\right\rangle $ is
simply the conjunction of the four global invariants.

I made a slight change to the client algorithm in that I\ expanded the scope
of the variable $i$ so that it can appear in the assertions both sides of the //.

To show that the proof outline is correct, we need to show that each assertion
is justified.

\begin{itemize}
\item $x=X\wedge q=0\wedge i=0$. This follows from the initializations.

\item \emph{The preconditions of the components. }We need to check that each
global invariant follows from%
\[
x=X\wedge q=0\wedge i=0
\]
and also that $i\leq N\wedge q=0$ does. These five implications are all
trivially universally true.

\item \emph{The postconditions of the components imply the overall
postcondition.} Combining the postcondition $q=2\wedge i=N$ with the global
invariant $q\neq1\Rightarrow x=h^{i}(X)$, we get $x=h^{N}(X)$.

\item \emph{Global invariants.} We show that the claimed global invariants are
preserved by each atomic action.

\begin{itemize}
\item $0\leq q\leq2$. The assignment actions we have to worry about are%
\begin{align*}
&  \left\langle x,q:=f(x),0;\right\rangle \qquad\text{,}\\
&  \left\langle x,q,i:=g(x),1,i+1;\right\rangle \qquad\text{, and}\\
&  \left\langle q:=2;\right\rangle
\end{align*}
The substituted postconditions are $0\leq0\leq2$, $0\leq1\leq2$, and
$0\leq2\leq2$ which are all trivially true.

\item $i\geq0$. By disjoint variables we only need to show%
\[
\left\{  i\geq0\right\}  \;\left\langle x,q,i:=g(x),1,i+1;\right\rangle
\;\left\{  i\geq0\right\}
\]
From the assignment rule, this follows from the fact that%
\[
i\geq0\Rightarrow i+1\geq0
\]
is universally true.\footnote{Well maybe and maybe not. If our \textsf{int}s
are true integers, there is no issue. If ints are bounded in range, it's not
universally true. We could use a larger subset of the precondition and show%
\[
i\geq0\wedge i<N\Rightarrow i+1\geq0
\]
Given that $N$ is an \textsf{int}, this will be universally trie for bounded
\textsf{int}s..}

\item $q\neq1\Rightarrow x=h^{i}(X)$.

\begin{itemize}
\item The assignment%
\[
\left\langle x,q,i:=g(x),1,i+1;\right\rangle
\]
sets $q$ to 1. Doing the substitution we get%
\[
1\neq1\Rightarrow...
\]
which we can see is true without considering what is in the ... , nor
considering the preconditions.

\item For the assignment%
\[
\left\langle x,q:=f(x),0;\right\rangle
\]
the substituted postcondition simplifies to $f(x)=h^{i}(X)$. The precondition
includes the conjuncts%
\[
q=1\wedge\left(  q=1\Rightarrow x=g(h^{i-1}(X))\wedge i>0\right)
\qquad\text{,}%
\]
which imply $x=g(h^{i-1}(X))\wedge i>0$. Now apply $f$ to both sides and we
get $f(x)=f(g(h^{i-1}(x))$, which is the same as $f(x)=h^{i}(X)$.

\item For the assignment $\left\langle q:=2;\right\rangle $, the substituted
postcondition simplifies to $x=h^{i}(X)$. The preconditions include%
\[
q=0\wedge\left(  q\neq1\Rightarrow x=h^{i}(X)\right)  \qquad\text{,}%
\]
which clearly imply $x=h^{i}(x)$.
\end{itemize}

\item $q=1\Rightarrow x=g(h^{i-1}(X))\wedge i>0$. The assignments
\begin{align*}
&  \left\langle x,q:=f(x),0;\right\rangle \qquad\text{and}\\
&  \left\langle q:=2;\right\rangle
\end{align*}
both give a substituted postcondition of the form%
\[
\mathrm{false}\Rightarrow...
\]
which is trivially true. The interesting action is $\left\langle
x,q,i:=g(x),1,i+1;\right\rangle $. The substituted postcondition simplifies
to
\[
g(x)=g(h^{i}(X))\wedge i+1>0\qquad\text{,}%
\]
which is implied by the following subset of the precondition:%
\[
q=0\wedge\left(  q\neq1\Rightarrow x=h^{i}(X)\right)  \wedge i\geq
0\qquad\text{.}%
\]

\end{itemize}

\item \emph{Local correctness.} Showing the that the global invariants really
are invariant is part of local correctness. What remains is to show the local
parts of each assertion follow from the preceding atomic actions. This is
straight forward in each case. It suffices to show%
\begin{align*}
&  \vdash\left\{  \mathrm{true}\right\}  \;\left\langle \mathbf{await}%
\;(q=1)\right\rangle \;\{q=1\}\qquad\text{,}\\
&  \vdash q=0\wedge i=0\Rightarrow0\leq i\leq N\wedge q=0\qquad\text{,}\\
&  \vdash0\leq i\leq N\wedge q=0\wedge i<N\Rightarrow0\leq i<N\wedge
q=0\qquad\text{,}\\
&  \vdash\left\{  0\leq i<N\right\}  \;\left\langle
x,q,i:=g(x),1,i+1;\right\rangle \;\left\{  0\leq i\leq N\right\}
\qquad\text{,}\\
&  \vdash\left\{  0\leq i\leq N\right\}  \;\left\langle \mathbf{await\;}%
(q=0)\right\rangle \;\left\{  0\leq i\leq N\wedge q=0\right\}  \qquad
\text{,}\\
&  \vdash0\leq i\leq N\wedge q=0\wedge\lnot\left(  i<N\right)  \Rightarrow
q=0\wedge i=N\qquad\text{, and}\\
&  \vdash\left\{  i=N\right\}  \;\left\langle q:=2;\right\rangle \;\left\{
q=2\wedge i=N\right\}
\end{align*}
In each case the reasoning is straight-forward.

\item \emph{Freedom from interference.} Showing the global invariants really
are invariant is part of showing freedom from interference. What remains is to
show that the following pairs don't interfere%
\[%
\begin{tabular}
[c]{|l|l|}\hline
$q=1$ & $\left\langle x,q,i:=g(x),1,i+1;\right\rangle $\\\hline
$q=1$ & $\left\langle q:=2;\right\rangle $\\\hline
$q=0$ & $\left\langle x,q:=f(x),0;\right\rangle $\\\hline
$q=0$ & $\left\langle x,q:=f(x),0;\right\rangle $\\\hline
\end{tabular}
\ \
\]
I've omitted all conjuncts that have to do with $i$ because they are free from
interference by reason of \textquotedblleft disjoint
variables\textquotedblright. Now let's deal with the pairs. In each case we
actually have mutual exclusion. For example, in the first, the precondition of
the action includes the conjunct $q=0$. Now, subsetting the precondition, it
suffices to show%
\[
\vdash\left\{  q=1\wedge q=0\right\}  \;\left\langle
x,q,i:=g(x),1,i+1;\right\rangle \;\left\{  q=1\right\}
\]
We need do no more work than to observe that the precondition is false. This
technique works in all four cases.

Three of the pairs (all but the second) can also be shown in another easy way.
Consider the first again. The substituted postcondition is $1=1$ which is
\textit{true}. We need not even look at the precondition. We have%
\[
\vdash\left\{  \mathrm{true}\right\}  \;\left\langle
x,q,i:=g(x),1,i+1;\right\rangle \;\left\{  q=1\right\}
\]
This is an extreme case of subsetting the precondition.
\end{itemize}

This concludes the example.

\section{Ghost Variables\label{ghost}}

A technique that is often useful and occasionally indispensable is that of
ghost variables. The idea is to introduce extra variables that are useful for
creating a correct proof outline, but that are not needed in the actual
implementation. These variables are called `\textbf{ghost variables.'} (Some
writers call them `auxiliary variables,' `thought variables,' or `dummy variables.')

Here is a simple example. Consider the algorithm

\begin{code}
\#\# $x=0$

\textbf{co}

\begin{indent}
\item $\langle$ $x:=x+1$; $\rangle$
\end{indent}

//

\begin{indent}
\item $\langle$ $x:=x+1$; $\rangle$
\end{indent}

\textbf{oc}

\#\# $x=2$
\end{code}

How can we complete the outline?\ If we put in a precondition for each
component of $x=0$, like this

\begin{code}
\#\# $x=0$

\textbf{co}

\begin{indent}
\item \#\# $x=0$

\item $\langle$ $x:=x+1$; $\rangle$
\end{indent}

//

\begin{indent}
\item \#\# $x=0$

\item $\langle$ $x:=x+1$; $\rangle$
\end{indent}

\textbf{oc}

\#\# $x=2$
\end{code}

\noindent there is interference. Weakening the preconditions to try to avoid
interference, like this

\begin{code}
\#\# $x=0$

\textbf{co}

\begin{indent}
\item \#\# $x=0\vee x=1$

\item $\langle$ $x:=x+1$; $\rangle$
\end{indent}

//

\begin{indent}
\item \#\# $x=0\vee x=1$

\item $\langle$ $x:=x+1$; $\rangle$
\end{indent}

\textbf{oc}

\#\# $x=2$
\end{code}

\noindent doesn't work; there is still interference. \emph{We} know that once
the other process has incremented $x$, it won't do it again, but the
definition of interference only considers the actions of the other process,
not the sequence or frequency that they might happen in.

We can introduce ghost variables $a$ and $b$ to track the local changes to
$x$; $a$ represents how much the first component has incremented $x$, while
$b$ represents how much the second component has incremented $x$. Thus
$x=a+b$, at all times. To emphasize that $a$ and $b$ are ghost variables, we
put their declarations in comments marked by \textquotedblleft%
\#\textquotedblright.

\begin{code}
\#\# $x=0$

\# int $a:=0$

\#\ int $b:=0$

\#\# $x=0\wedge a=0\wedge b=0$

\#\ Global invariant: $x=a+b$

\textbf{co}

\begin{indent}
\item \#\# $a=0$

\item $\langle$ $x:=x+1$; $a:=1;\rangle$

\item \#\# $a=1$
\end{indent}

//

\begin{indent}
\item \#\# $b=0$

\item $\langle$ $x:=x+1$; $b:=1;\rangle$

\item \#\# $b=1$
\end{indent}

\textbf{oc}

\#\# $x=2$
\end{code}

\noindent By disjoint variables, there is no interference. We need only show
that each action is correct locally and that each maintains the global
invariant. The final assertion that $x=2$ follows from the global invariant
together with the assertions $a=1$ and $b=1$.

\section{Data refinement\label{trans}}

In engineering, we often replace an abstract part of a design by something
more concrete that refines it. For example, an early version of the design of
a building might specify a column capable of transmitting a given force. Later
in the design process, that column might be replaced by a set of smaller
columns that together do the same job.

In programming we can replace one set of variables with another set of
variables. This process is known as data refinement. Data refinement allows us
to go from abstract solutions that are easily seen to be correct, but that may
be difficult to implement, to concrete solutions that are easy to implement,
but whose correctness may be less obvious. It is a useful technique in
concurrent programming and also in sequential programming.

Data refinement is best done in three stages:

\begin{description}
\item[Augment] First, we introduce the new set of variables, linked to the
other variables by an invariant.

\item[Transform] Second, we transform the program until (some of) the original
variables are no longer needed.

\item[Diminish] Finally, we eliminate any variables that are no longer needed,
or demote them to the status of ghost variables.
\end{description}

As an example, consider a solution to the mutual exclusion problem. We start
with a program that ensures that only one of commands $A$, $B$, and $C$ are
being executed at any one time. We assume that $A$, $B$, and $C$ don't change
variables $a$, $b$, or $c$.

\begin{code}
int $a:=0,b:=0,c:=0$ ;

\#\# $a=0\wedge b=0\wedge c=0$

\#\ Global invariant: $0\leq a,b,c\leq1$

\#\ Global invariant: $a+b+c<2$

\textbf{co}

\begin{indent}


\item \textbf{while(} \textit{true} ) \{

\begin{indent}
\item $\left\langle \mathbf{await}(a+b+c=0)\;a:=1;\right\rangle $

\item $A$

\item $\left\langle a:=0;\right\rangle $ \}
\end{indent}
\end{indent}

//

\begin{indent}
\item \textbf{while(} \textit{true} ) \{

\begin{indent}
\item $\left\langle \mathbf{await}(a+b+c=0)\;b:=1;\right\rangle $

\item $B$

\item $\left\langle b:=0;\right\rangle $ \}
\end{indent}
\end{indent}

//

\begin{indent}
\item \textbf{while(} \textit{true} ) \{

\begin{indent}
\item $\left\langle \mathbf{await}(a+b+c=0)\;c:=1;\right\rangle $

\item $C$

\item $\left\langle c:=0;\right\rangle $ \}
\end{indent}
\end{indent}

\textbf{oc}
\end{code}

The problem with this program is that the await commands must read three
variables at once, which might be difficult to engineer.

\textbf{Augmenting:} Let's introduce a new variable $s$, tied to $a$, $b$, and
$c$ by a global invariant $s=a+b+c$.

\begin{code}
int $a:=0,b:=0,c:=0$ ;

int $s:=0$ ;

\#\# $a=0\wedge b=0\wedge c=0\wedge s=0$

\#\ Global invariant: $0\leq a,b,c\leq1$

\#\ Global invariant: $a+b+c<2$

\#\ Global invariant: $s=a+b+c$

\textbf{co}

\begin{indent}
\item \textbf{while(} \textit{true} ) \{

\begin{indent}
\item $\left\langle \mathbf{await}(a+b+c=0)\;a:=1;s:=1;\right\rangle $

\item $A$

\item $\left\langle a:=0;s:=0;\right\rangle $ \}
\end{indent}
\end{indent}

//

\begin{indent}
\item \textbf{while(} \textit{true} ) \{

\begin{indent}
\item $\left\langle \mathbf{await}(a+b+c=0)\;b:=1;s:=1;\right\rangle $

\item $B$

\item $\left\langle b:=0;s:=0;\right\rangle $ \}
\end{indent}
\end{indent}

//

\begin{indent}
\item \textbf{while(} \textit{true} ) \{

\begin{indent}
\item $\left\langle \mathbf{await}(a+b+c=0)\;c:=1;s:=1;\right\rangle $

\item $C$

\item $\left\langle c:=0;s:=0;\right\rangle $ \}
\end{indent}
\end{indent}

\textbf{oc}
\end{code}

\textbf{Transforming:}\ Now, by the global invariant $s=a+b+c$, we can replace
the expression $a+b+c$ with $s$ anywhere it appears:

\begin{code}
int $a:=0,b:=0,c:=0$ ;

int $s:=0$ ;

\#\# $a=0\wedge b=0\wedge c=0\wedge s=0$

\#\ Global invariant: $0\leq a,b,c\leq1$

\#\ Global invariant: $0\leq s\leq1$

\#\ Global invariant: $s=a+b+c$

\textbf{co}

\begin{indent}
\item \textbf{while(} \textit{true} ) \{

\begin{indent}
\item $\left\langle \mathbf{await}(s=0)\;a:=1;s:=1;\right\rangle $

\item $A$

\item $\left\langle a:=0;s:=0;\right\rangle $ \}
\end{indent}
\end{indent}

//

\begin{indent}
\item \textbf{while(} \textit{true} ) \{

\begin{indent}
\item $\left\langle \mathbf{await}(s=0)\;b:=1;s:=1;\right\rangle $

\item $B$

\item $\left\langle b:=0;s:=0;\right\rangle $ \}
\end{indent}
\end{indent}

//

\begin{indent}
\item \textbf{while(} \textit{true} ) \{

\begin{indent}
\item $\left\langle \mathbf{await}(s=0)\;c:=1;s:=1;\right\rangle $

\item $C$

\item $\left\langle c:=0;s:=0;\right\rangle $ \}
\end{indent}
\end{indent}

\textbf{oc}
\end{code}

\textbf{Diminishing:}\ At this point, variables $a$, $b$, and $c$ are not used
in the algorithm; we can demote $a$, $b,$ and $c$ to the status of ghosts, or
eliminate them altogether.

\begin{code}
int $s:=0$ ;

\#\# $s=0$

\#\ Global invariant: $0\leq s\leq1$

\textbf{co}

\begin{indent}
\item \textbf{while(} \textit{true} ) \{

\begin{indent}
\item $\left\langle \mathbf{await}(s=0)\;s:=1;\right\rangle $

\item $A$

\item $\left\langle s:=0;\right\rangle $ \}
\end{indent}
\end{indent}

//

\begin{indent}
\item \textbf{while(} \textit{true} ) \{

\begin{indent}
\item $\left\langle \mathbf{await}(s=0)\;s:=1;\right\rangle $

\item $B$

\item $\left\langle s:=0;\right\rangle $ \}
\end{indent}
\end{indent}

//

\begin{indent}
\item \textbf{while(} \textit{true} ) \{

\begin{indent}
\item $\left\langle \mathbf{await}(s=0)\;s:=1;\right\rangle $

\item $C$

\item $\left\langle s:=0;\right\rangle $ \}
\end{indent}
\end{indent}

\textbf{oc}
\end{code}

The command $\left\langle \mathbf{await}(s=0)\;s:=1;\right\rangle $ can easily
be implemented with the test-and-set instruction found on many computers.

\bibliographystyle{named}
\bibliography{big}



\end{document}
